{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uTv0D26B9W2h"
   },
   "source": [
    "# Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IdN5pnC8MMx-"
   },
   "source": [
    "This notebook is intended to produce the plots and figures for the report on Problem 1 of the practical. You should not run this notebook in Google Colab until you have finished constructing the correct solutions for transformer_solution.py and encoder_decoder_solution.py\n",
    "\n",
    "This notebook provides some limited commentary on several HuggingFace Features and toolage. You will use HuggingFace Datasets to load the Amazon Polarity dataset for sentiment analysis. The notebook will define a Bert tokenizer, collate functions, and then train and evaluate several models using the HuggingFace utilities mentioned above. Remember, the most crucial part here is running the experiments for the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mount your Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qFHMMDtSwuW4",
    "outputId": "8db247e6-4231-4915-d44a-1c4e30c72766"
   },
   "outputs": [],
   "source": [
    "# If you run this notebook locally or on a cluster (i.e. not on Google Colab)\n",
    "# you can delete this cell which is specific to Google Colab. You may also\n",
    "# change the paths for data/logs in Arguments below.\n",
    "\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')\n",
    "\n",
    "# !pip install -qqq datasets transformers textattack --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Link your assignment folder & install requirements\n",
    "Enter the path to the assignment folder in your Google Drive\n",
    "If you run this notebook locally or on a cluster (i.e. not on Google Colab)\n",
    "you can delete this cell which is specific to Google Colab. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "oODLwt1QzgGa"
   },
   "outputs": [],
   "source": [
    "#folder = \"\" #@param {type:\"string\"}\n",
    "#!ln -Ts \"$folder\" /content/assignment 2> /dev/null\n",
    "#!cp gdrive/MyDrive/Assignment2/transformer.py .\n",
    "#!cp gdrive/MyDrive/Assignment2/lstm.py .\n",
    "\n",
    "# Add the assignment folder to Python path\n",
    "#if '/content/assignment' not in sys.path:\n",
    "#  sys.path.insert(0, '/content/assignment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8142/1081942771.py:10: UserWarning: CUDA is not available.\n",
      "  warnings.warn('CUDA is not available.')\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "import warnings\n",
    "import json\n",
    "\n",
    "# Check if CUDA is available\n",
    "import torch\n",
    "if not torch.cuda.is_available():\n",
    "  warnings.warn('CUDA is not available.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "dt3NTvpsy4Oc",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Running on GPU\n",
    "For this assignment, it will be necessary to run your experiments on GPU. To make sure the notebook is running on GPU, you can change the notebook settings with\n",
    "* (EN) `Edit > Notebook Settings`\n",
    "* (FR) `Modifier > Paramètres du notebook`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "RLVSmv9HoMH5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib.request\n",
    "from sklearn.metrics import f1_score   \n",
    "import time\n",
    "\n",
    "from typing import List, Dict, Union, Optional, Tuple\n",
    "import torch\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "from datasets import Dataset\n",
    "import transformers\n",
    "\n",
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "from transformer import Transformer, MultiHeadedAttention\n",
    "from lstm import EncoderDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = load_dataset(\"yelp_polarity\", split=\"train\", cache_dir=\"assignment/data\")\n",
    "dataset_test = load_dataset(\"yelp_polarity\", split=\"test[:1000]\", cache_dir=\"assignment/data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🔍 Quick look at the data\n",
    "Lets have quick look at a few samples in our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "title: Contrary to other reviews, I have zero complaints about the service or the prices. I have been getting tire service here for the past 5 years now, and compared to my experience with places like Pep Boys, these guys are experienced and know what they're doing. \\nAlso, this is one place that I do not feel like I am being taken advantage of, just because of my gender. Other auto mechanics have been notorious for capitalizing on my ignorance of cars, and have sucked my bank account dry. But here, my service and road coverage has all been well explained - and let up to me to decide. \\nAnd they just renovated the waiting room. It looks a lot better than it did in previous years.\n",
      "label: 1\n",
      "------------------------------\n",
      "title: Last summer I had an appointment to get new tires and had to wait a super long time. I also went in this week for them to fix a minor problem with a tire they put on. They \\\"\"fixed\\\"\" it for free, and the very next morning I had the same issue. I called to complain, and the \\\"\"manager\\\"\" didn't even apologize!!! So frustrated. Never going back.  They seem overpriced, too.\n",
      "label: 0\n",
      "------------------------------\n",
      "title: Friendly staff, same starbucks fair you get anywhere else.  Sometimes the lines can get long.\n",
      "label: 1\n"
     ]
    }
   ],
   "source": [
    "n_samples_to_see = 3\n",
    "for i in range(n_samples_to_see):\n",
    "  print(\"-\"*30)\n",
    "  print(\"title:\", dataset_test[i][\"text\"])\n",
    "  print(\"label:\", dataset_test[i][\"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fzxwRDQFUtaG"
   },
   "source": [
    "### 1️. Tokenize the `text`\n",
    "Tokenize the `text`portion of each sample (i.e. parsing the text to smaller chunks). Tokenization can happen in many ways; traditionally, this was done based on the white spaces. With transformer-based models, tokenization is performed based on the frequency of occurrence of \"chunk of text\". This frequency can be learned in many different ways. However the most common one is the [**wordpiece**](https://arxiv.org/pdf/1609.08144v2.pdf) model. \n",
    "> The wordpiece model is generated using a data-driven approach to maximize the language-model likelihood\n",
    "of the training data, given an evolving word definition. Given a training corpus and a number of desired\n",
    "tokens $D$, the optimization problem is to select $D$ wordpieces such that the resulting corpus is minimal in the\n",
    "number of wordpieces when segmented according to the chosen wordpiece model.\n",
    "\n",
    "Under this model:\n",
    "1. Not all things can be converted to tokens depending on the model. For example, most models have been pretrained without any knowledge of emojis. So their token will be `[UNK]`, which stands for unknown.\n",
    "2. Some words will be mapped to multiple tokens!\n",
    "3. Depending on the kind of model, your tokens may or may not respect capitalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "qCpNwaTYSo3U"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3OMDqabyToBt",
    "outputId": "f3477698-69e0-41e6-c840-beca5796e5c3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['welcome',\n",
       " 'to',\n",
       " 'if',\n",
       " '##t',\n",
       " '##6',\n",
       " '##13',\n",
       " '##5',\n",
       " '.',\n",
       " 'we',\n",
       " 'now',\n",
       " 'teach',\n",
       " 'you',\n",
       " '[UNK]',\n",
       " '(',\n",
       " 'hugging',\n",
       " 'face',\n",
       " ')',\n",
       " 'library',\n",
       " ':',\n",
       " 'dd',\n",
       " '##d',\n",
       " '.']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sample = \"Welcome to IFT6135. We now teach you 🤗(HUGGING FACE) Library :DDD.\"\n",
    "tokenizer.tokenize(input_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NEu6aqReXqp6"
   },
   "source": [
    "### 2. Encoding\n",
    "Once we have tokenized the text, we then need to convert these chuncks to numbers so we can feed them to our model. This conversion is basically a look-up in a dictionary **from `str` $\\to$ `int`**. The tokenizer object can also perform this work. While it does so it will also add the *special* tokens needed by the model to the encodings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EpDGccrvYKnT",
    "outputId": "f5776da3-28fa-4fc2-d247-d231214395e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Token Encodings:\n",
      " [101, 6160, 2000, 2065, 2102, 2575, 17134, 2629, 1012, 2057, 2085, 6570, 2017, 100, 1006, 17662, 2227, 1007, 3075, 1024, 20315, 2094, 1012, 102]\n",
      "-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.\n",
      "--> Token Encodings Decoded:\n",
      " [CLS] welcome to ift6135. we now teach you [UNK] ( hugging face ) library : ddd. [SEP]\n"
     ]
    }
   ],
   "source": [
    "input_sample = \"Welcome to IFT6135. We now teach you 🤗(HUGGING FACE) Library :DDD.\" #@param {type: \"string\"}\n",
    "\n",
    "print(\"--> Token Encodings:\\n\",tokenizer.encode(input_sample))\n",
    "print(\"-.\"*15)\n",
    "print(\"--> Token Encodings Decoded:\\n\",tokenizer.decode(tokenizer.encode(input_sample)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DI8lFKZSZ2ZW"
   },
   "source": [
    "### 3️. Truncate/Pad samples\n",
    "Since all the sample in the batch will not have the same sequence length, we would need to truncate the longer sequences (i.e. the ones that exeed a predefined maximum length) and pad the shorter ones so we that we can equal length for all the samples in the batch. Once this is achieved, we would need to convert the result to `torch.Tensor`s and return. These tensors will then be retrieved from the [dataloader](https://https//pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Collate:\n",
    "    def __init__(self, tokenizer: str, max_len: int) -> None:\n",
    "        self.tokenizer_name = tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.tokenizer_name)\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __call__(self, batch: List[Dict[str, Union[str, int]]]) -> Dict[str, torch.Tensor]:\n",
    "        texts = list(map(lambda batch_instance: batch_instance[\"text\"], batch))\n",
    "        tokenized_inputs = self.tokenizer(\n",
    "            texts,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            return_tensors=\"pt\",\n",
    "            return_token_type_ids=False,\n",
    "        )\n",
    "        \n",
    "        labels = list(map(lambda batch_instance: int(batch_instance[\"label\"]), batch))\n",
    "        labels = torch.LongTensor(labels)\n",
    "        return dict(tokenized_inputs, **{\"labels\": labels})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 🧑‍🍳 Setting up the collate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "4VaSpuyIjNqn"
   },
   "outputs": [],
   "source": [
    "tokenizer_name = \"bert-base-uncased\"\n",
    "sample_max_length = 256\n",
    "collate = Collate(tokenizer=tokenizer_name, max_len=sample_max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "y9P4oWyOSexA"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "import torch.nn as nn\n",
    "torch.random.manual_seed(0)\n",
    "\n",
    "class ReviewClassifier(nn.Module):\n",
    "    def __init__(self, backbone: str, backbone_hidden_size: int, nb_classes: int):\n",
    "        super(ReviewClassifier, self).__init__()\n",
    "        self.backbone = backbone\n",
    "        self.backbone_hidden_size = backbone_hidden_size\n",
    "        self.nb_classes = nb_classes\n",
    "        self.back_bone = AutoModel.from_pretrained(\n",
    "            self.backbone,\n",
    "            output_attentions=False,\n",
    "            output_hidden_states=False,\n",
    "        )\n",
    "        self.classifier = torch.nn.Linear(self.backbone_hidden_size, self.nb_classes)\n",
    "\n",
    "    def forward(\n",
    "        self, input_ids: torch.Tensor, attention_mask: torch.Tensor, labels: Optional[torch.Tensor] = None\n",
    "    ) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n",
    "        back_bone_output = self.back_bone(input_ids, attention_mask=attention_mask)\n",
    "        hidden_states = back_bone_output[0]\n",
    "        pooled_output = hidden_states[:, 0]  # getting the [CLS] token\n",
    "        logits = self.classifier(pooled_output)\n",
    "        if labels is not None:\n",
    "            loss_fn = torch.nn.CrossEntropyLoss()\n",
    "            loss = loss_fn(\n",
    "                logits.view(-1, self.nb_classes),\n",
    "                labels.view(-1),\n",
    "            )\n",
    "            return loss, logits\n",
    "        return logits\n",
    "\n",
    "class ReviewClassifierLSTM(nn.Module):\n",
    "    def __init__(self, nb_classes: int, encoder_only: bool = False, \n",
    "        with_attn: bool = True, dropout: int = 0.5, hidden_size: int = 256):\n",
    "        super(ReviewClassifierLSTM, self).__init__()\n",
    "        self.nb_classes = nb_classes\n",
    "        self.encoder_only = encoder_only\n",
    "\n",
    "        if with_attn:\n",
    "            attn = MultiHeadedAttention(head_size = 2*hidden_size, num_heads=1)\n",
    "        else:\n",
    "            attn = None\n",
    "            \n",
    "        self.back_bone = EncoderDecoder(dropout=dropout, encoder_only=encoder_only,\n",
    "                                        attn=attn, hidden_size=hidden_size)\n",
    "        \n",
    "        if self.encoder_only:\n",
    "            self.classifier = torch.nn.Linear(hidden_size*2, self.nb_classes)\n",
    "        else:\n",
    "            self.classifier = torch.nn.Linear(hidden_size, self.nb_classes)\n",
    "       \n",
    "    def forward(\n",
    "        self, input_ids: torch.Tensor, attention_mask: torch.Tensor, labels: Optional[torch.Tensor] = None\n",
    "    ) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n",
    "        pooled_output, _ = self.back_bone(input_ids, attention_mask)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        if labels is not None:\n",
    "            loss_fn = torch.nn.CrossEntropyLoss()\n",
    "            loss = loss_fn(\n",
    "                logits.view(-1, self.nb_classes),\n",
    "                labels.view(-1),\n",
    "            )\n",
    "            return loss, logits\n",
    "        return logits\n",
    "\n",
    "\n",
    "class ReviewClassifierTransformer(nn.Module):\n",
    "    def __init__(self, nb_classes: int, num_heads: int = 4, num_layers: int = 4, block: str=\"prenorm\", dropout: float = 0.3):\n",
    "        super(ReviewClassifierTransformer, self).__init__()\n",
    "        self.nb_classes = nb_classes\n",
    "        self.back_bone = Transformer(num_heads=num_heads, num_layers=num_layers, block=block, dropout=dropout)\n",
    "        self.classifier = torch.nn.Linear(256, self.nb_classes)\n",
    "\n",
    "    def forward(\n",
    "        self, input_ids: torch.Tensor, attention_mask: torch.Tensor, labels: Optional[torch.Tensor] = None\n",
    "    ) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n",
    "        attention_mask = torch.cat([torch.ones(attention_mask.shape[0]).unsqueeze(1).to(device),\n",
    "                                    attention_mask], dim=1)\n",
    "        back_bone_output = self.back_bone(input_ids, attention_mask)\n",
    "        hidden_states = back_bone_output\n",
    "        pooled_output = hidden_states\n",
    "        logits = self.classifier(pooled_output)\n",
    "        if labels is not None:\n",
    "            loss_fn = torch.nn.CrossEntropyLoss()\n",
    "            loss = loss_fn(\n",
    "                logits.view(-1, self.nb_classes),\n",
    "                labels.view(-1),\n",
    "            )\n",
    "            return loss, logits\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "HP58LrWUjFt4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Device selected: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f\"--> Device selected: {device}\")\n",
    "def train_one_epoch(\n",
    "    model: torch.nn.Module, training_data_loader: DataLoader, optimizer: torch.optim.Optimizer, logging_frequency: int, testing_data_loader: DataLoader, logger: dict):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    epoch_loss = 0\n",
    "    logging_loss = 0\n",
    "    start_time = time.time()\n",
    "    mini_start_time = time.time()\n",
    "    for step, batch in enumerate(training_data_loader):\n",
    "        batch = {key: value.to(device) for key, value in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs[0]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        logging_loss += loss.item()\n",
    "\n",
    "        if (step + 1) % logging_frequency == 0:\n",
    "            freq_time = time.time()-mini_start_time\n",
    "            logger['train_time'].append(freq_time+logger['train_time'][-1])\n",
    "            logger['train_losses'].append(logging_loss/logging_frequency)\n",
    "            print(f\"Training loss @ step {step+1}: {logging_loss/logging_frequency}\")\n",
    "            eval_acc, eval_f1, eval_loss, eval_time = evaluate(model, testing_data_loader)\n",
    "            logger['eval_accs'].append(eval_acc)\n",
    "            logger['eval_f1s'].append(eval_f1)\n",
    "            logger['eval_losses'].append(eval_loss)\n",
    "            logger['eval_time'].append(eval_time+logger['eval_time'][-1])\n",
    "\n",
    "            logging_loss = 0\n",
    "            mini_start_time = time.time()\n",
    "\n",
    "    return epoch_loss / len(training_data_loader), time.time()-start_time\n",
    "\n",
    "\n",
    "def evaluate(model: torch.nn.Module, test_data_loader: DataLoader):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    eval_loss = 0\n",
    "    correct_predictions = {i: 0 for i in range(2)}\n",
    "    total_predictions = {i: 0 for i in range(2)}\n",
    "    preds = []\n",
    "    targets = []\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        for step, batch in enumerate(test_data_loader):\n",
    "            batch = {key: value.to(device) for key, value in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs[0]\n",
    "            eval_loss += loss.item()\n",
    "\n",
    "            predictions = np.argmax(outputs[1].detach().cpu().numpy(), axis=1)\n",
    "            preds.extend(predictions.tolist())\n",
    "            targets.extend(batch[\"labels\"].cpu().numpy().tolist())\n",
    "\n",
    "            for target, prediction in zip(batch[\"labels\"].cpu().numpy(), predictions):\n",
    "                if target == prediction:\n",
    "                    correct_predictions[target] += 1\n",
    "                total_predictions[target] += 1\n",
    "    accuracy = (100.0 * sum(correct_predictions.values())) / sum(total_predictions.values())\n",
    "    f1 = f1_score(targets, preds)\n",
    "    model.train()\n",
    "    return accuracy, round(f1, 4), eval_loss / len(test_data_loader), time.time() - start_time\n",
    "\n",
    "\n",
    "def save_logs(dictionary, log_dir, exp_id):\n",
    "  log_dir = os.path.join(log_dir, exp_id)\n",
    "  os.makedirs(log_dir, exist_ok=True)\n",
    "  # Log arguments\n",
    "  with open(os.path.join(log_dir, \"args.json\"), \"w\") as f:\n",
    "    json.dump(dictionary, f, indent=2)\n",
    "\n",
    "def save_model(model, log_dir, exp_id):\n",
    "  log_dir = os.path.join(log_dir, exp_id)\n",
    "  os.makedirs(log_dir, exist_ok=True)\n",
    "  # Save model\n",
    "  torch.save(model.state_dict(), f\"assignment/models/model_{exp_id}.pt\")\n",
    "\n",
    "batch_size = 512\n",
    "\n",
    "train_loader = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, collate_fn=collate)\n",
    "test_loader = DataLoader(dataset_test, batch_size=batch_size, shuffle=False, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Problem 3\n",
    "Feel free to modify this code however it is convenient for you to produce a report except for the model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting 8: Fine-tuning BERT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17b9e0005fba41618d4a4ea641c5ba1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/teamspace/studios/this_studio/ift6135_assignment_2/main_lightning.ipynb Cell 28\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-01hsfpb68hhr7rrkm1frvncms5.studio.lightning.ai/teamspace/studios/this_studio/ift6135_assignment_2/main_lightning.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=59'>60</a>\u001b[0m     \u001b[39mif\u001b[39;00m name\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39mencoder.layer.11\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-01hsfpb68hhr7rrkm1frvncms5.studio.lightning.ai/teamspace/studios/this_studio/ift6135_assignment_2/main_lightning.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=60'>61</a>\u001b[0m         param\u001b[39m.\u001b[39mrequires_grad \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://vscode-01hsfpb68hhr7rrkm1frvncms5.studio.lightning.ai/teamspace/studios/this_studio/ift6135_assignment_2/main_lightning.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=61'>62</a>\u001b[0m train_loss, train_time \u001b[39m=\u001b[39m train_one_epoch(model, train_loader, optimizer, logging_frequency, test_loader, logger)\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-01hsfpb68hhr7rrkm1frvncms5.studio.lightning.ai/teamspace/studios/this_studio/ift6135_assignment_2/main_lightning.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=62'>63</a>\u001b[0m eval_acc, eval_f1, eval_loss, eval_time  \u001b[39m=\u001b[39m evaluate(model, test_loader)\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-01hsfpb68hhr7rrkm1frvncms5.studio.lightning.ai/teamspace/studios/this_studio/ift6135_assignment_2/main_lightning.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=63'>64</a>\u001b[0m logger[\u001b[39m\"\u001b[39m\u001b[39mepoch_train_loss\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mappend(train_loss)\n",
      "\u001b[1;32m/teamspace/studios/this_studio/ift6135_assignment_2/main_lightning.ipynb Cell 28\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-01hsfpb68hhr7rrkm1frvncms5.studio.lightning.ai/teamspace/studios/this_studio/ift6135_assignment_2/main_lightning.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mfor\u001b[39;00m step, batch \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(training_data_loader):\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-01hsfpb68hhr7rrkm1frvncms5.studio.lightning.ai/teamspace/studios/this_studio/ift6135_assignment_2/main_lightning.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m     batch \u001b[39m=\u001b[39m {key: value\u001b[39m.\u001b[39mto(device) \u001b[39mfor\u001b[39;00m key, value \u001b[39min\u001b[39;00m batch\u001b[39m.\u001b[39mitems()}\n\u001b[0;32m---> <a href='vscode-notebook-cell://vscode-01hsfpb68hhr7rrkm1frvncms5.studio.lightning.ai/teamspace/studios/this_studio/ift6135_assignment_2/main_lightning.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m     outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mbatch)\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-01hsfpb68hhr7rrkm1frvncms5.studio.lightning.ai/teamspace/studios/this_studio/ift6135_assignment_2/main_lightning.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m     loss \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-01hsfpb68hhr7rrkm1frvncms5.studio.lightning.ai/teamspace/studios/this_studio/ift6135_assignment_2/main_lightning.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32m/teamspace/studios/this_studio/ift6135_assignment_2/main_lightning.ipynb Cell 28\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-01hsfpb68hhr7rrkm1frvncms5.studio.lightning.ai/teamspace/studios/this_studio/ift6135_assignment_2/main_lightning.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-01hsfpb68hhr7rrkm1frvncms5.studio.lightning.ai/teamspace/studios/this_studio/ift6135_assignment_2/main_lightning.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m     \u001b[39mself\u001b[39m, input_ids: torch\u001b[39m.\u001b[39mTensor, attention_mask: torch\u001b[39m.\u001b[39mTensor, labels: Optional[torch\u001b[39m.\u001b[39mTensor] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-01hsfpb68hhr7rrkm1frvncms5.studio.lightning.ai/teamspace/studios/this_studio/ift6135_assignment_2/main_lightning.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Union[torch\u001b[39m.\u001b[39mTensor, Tuple[torch\u001b[39m.\u001b[39mTensor, torch\u001b[39m.\u001b[39mTensor]]:\n\u001b[0;32m---> <a href='vscode-notebook-cell://vscode-01hsfpb68hhr7rrkm1frvncms5.studio.lightning.ai/teamspace/studios/this_studio/ift6135_assignment_2/main_lightning.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m     back_bone_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mback_bone(input_ids, attention_mask\u001b[39m=\u001b[39;49mattention_mask)\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-01hsfpb68hhr7rrkm1frvncms5.studio.lightning.ai/teamspace/studios/this_studio/ift6135_assignment_2/main_lightning.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m     hidden_states \u001b[39m=\u001b[39m back_bone_output[\u001b[39m0\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-01hsfpb68hhr7rrkm1frvncms5.studio.lightning.ai/teamspace/studios/this_studio/ift6135_assignment_2/main_lightning.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m     pooled_output \u001b[39m=\u001b[39m hidden_states[:, \u001b[39m0\u001b[39m]  \u001b[39m# getting the [CLS] token\u001b[39;00m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:1013\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1004\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m   1006\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[1;32m   1007\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   1008\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1011\u001b[0m     past_key_values_length\u001b[39m=\u001b[39mpast_key_values_length,\n\u001b[1;32m   1012\u001b[0m )\n\u001b[0;32m-> 1013\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m   1014\u001b[0m     embedding_output,\n\u001b[1;32m   1015\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[1;32m   1016\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1017\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m   1018\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[1;32m   1019\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1020\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1021\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1022\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1023\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1024\u001b[0m )\n\u001b[1;32m   1025\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1026\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:607\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    596\u001b[0m     layer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    597\u001b[0m         layer_module\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m,\n\u001b[1;32m    598\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    604\u001b[0m         output_attentions,\n\u001b[1;32m    605\u001b[0m     )\n\u001b[1;32m    606\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 607\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m    608\u001b[0m         hidden_states,\n\u001b[1;32m    609\u001b[0m         attention_mask,\n\u001b[1;32m    610\u001b[0m         layer_head_mask,\n\u001b[1;32m    611\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    612\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    613\u001b[0m         past_key_value,\n\u001b[1;32m    614\u001b[0m         output_attentions,\n\u001b[1;32m    615\u001b[0m     )\n\u001b[1;32m    617\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    618\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:497\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    486\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    487\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    494\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[1;32m    495\u001b[0m     \u001b[39m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    496\u001b[0m     self_attn_past_key_value \u001b[39m=\u001b[39m past_key_value[:\u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 497\u001b[0m     self_attention_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[1;32m    498\u001b[0m         hidden_states,\n\u001b[1;32m    499\u001b[0m         attention_mask,\n\u001b[1;32m    500\u001b[0m         head_mask,\n\u001b[1;32m    501\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    502\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mself_attn_past_key_value,\n\u001b[1;32m    503\u001b[0m     )\n\u001b[1;32m    504\u001b[0m     attention_output \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    506\u001b[0m     \u001b[39m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:436\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    418\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    419\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    425\u001b[0m     output_attentions: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    426\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[1;32m    427\u001b[0m     self_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mself(\n\u001b[1;32m    428\u001b[0m         hidden_states,\n\u001b[1;32m    429\u001b[0m         attention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    434\u001b[0m         output_attentions,\n\u001b[1;32m    435\u001b[0m     )\n\u001b[0;32m--> 436\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutput(self_outputs[\u001b[39m0\u001b[39;49m], hidden_states)\n\u001b[1;32m    437\u001b[0m     outputs \u001b[39m=\u001b[39m (attention_output,) \u001b[39m+\u001b[39m self_outputs[\u001b[39m1\u001b[39m:]  \u001b[39m# add attentions if we output them\u001b[39;00m\n\u001b[1;32m    438\u001b[0m     \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:388\u001b[0m, in \u001b[0;36mBertSelfOutput.forward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    386\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdense(hidden_states)\n\u001b[1;32m    387\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(hidden_states)\n\u001b[0;32m--> 388\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mLayerNorm(hidden_states \u001b[39m+\u001b[39;49m input_tensor)\n\u001b[1;32m    389\u001b[0m \u001b[39mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py:1507\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1504\u001b[0m             tracing_state\u001b[39m.\u001b[39mpop_scope()\n\u001b[1;32m   1505\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n\u001b[0;32m-> 1507\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_wrapped_call_impl\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m   1508\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1509\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "logging_frequency = 100\n",
    "learning_rate = 1e-5\n",
    "nb_epoch=2\n",
    "\n",
    "for i in range(8, 9):\n",
    "  experimental_setting = i\n",
    "\n",
    "  if experimental_setting == 1:\n",
    "    print(\"Setting 1: LSTM, no dropout, encoder only\")\n",
    "    model = ReviewClassifierLSTM(nb_classes=2, dropout=0, encoder_only=True)\n",
    "  if experimental_setting == 2:\n",
    "    print(\"Setting 2: LSTM, dropout, encoder only\")\n",
    "    model = ReviewClassifierLSTM(nb_classes=2, dropout=0.3, encoder_only=True)\n",
    "  if experimental_setting == 3:\n",
    "    print(\"Setting 3: LSTM, dropout, encoder-decoder, no attention\")\n",
    "    model = ReviewClassifierLSTM(nb_classes=2, dropout=0.3, encoder_only=False, with_attn=False)\n",
    "  if experimental_setting == 4:\n",
    "    print(\"Setting 4: LSTM, dropout, encoder-decoder, with attention\")\n",
    "    model = ReviewClassifierLSTM(nb_classes=2, dropout=0.3, encoder_only=False, with_attn=True)\n",
    "  if experimental_setting == 5:\n",
    "    print(\"Setting 5: Transformer, 2 layers, pre-normalization\")\n",
    "    model = ReviewClassifierTransformer(nb_classes=2, num_heads=4, num_layers=2, block='prenorm', dropout=0.3)\n",
    "  if experimental_setting == 6:\n",
    "    print(\"Setting 6: Transformer, 4 layers, pre-normalization\")\n",
    "    model = ReviewClassifierTransformer(nb_classes=2, num_heads=4, num_layers=4, block='prenorm', dropout=0.3)\n",
    "  if experimental_setting == 7:\n",
    "    print(\"Setting 7: Transformer, 2 layers, post-normalization\")\n",
    "    model = ReviewClassifierTransformer(nb_classes=2, num_heads=4, num_layers=2, block='postnorm', dropout=0.3)\n",
    "  if experimental_setting == 8:\n",
    "    nb_epoch = 2\n",
    "    print(\"Setting 8: Fine-tuning BERT\")\n",
    "    model = ReviewClassifier(backbone=\"bert-base-uncased\", backbone_hidden_size=768, nb_classes=2)\n",
    "    for parameter in model.back_bone.parameters():\n",
    "      parameter.requires_grad= False\n",
    "\n",
    "\n",
    "  # setting up the optimizer\n",
    "  optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=learning_rate, eps=1e-8)\n",
    "  model.to(device)\n",
    "  logger = dict()\n",
    "  logger['train_time'] = [0]\n",
    "  logger['eval_time'] = [0]\n",
    "  logger['train_losses'] = []\n",
    "  logger['eval_accs'] = []\n",
    "  logger['eval_f1s'] = []\n",
    "  logger['eval_losses'] = []\n",
    "  logger[\"epoch_train_loss\"] = []\n",
    "  logger[\"epoch_train_time\"] = []\n",
    "  logger[\"epoch_eval_loss\"] = []\n",
    "  logger[\"epoch_eval_time\"] = []\n",
    "  logger[\"epoch_eval_acc\"] = []\n",
    "  logger[\"epoch_eval_f1\"] = []\n",
    "  \n",
    "  logger['parameters'] = sum([p.numel() for p in model.back_bone.parameters() if p.requires_grad])\n",
    "\n",
    "  for epoch in range(nb_epoch):\n",
    "    print(f\"Epoch {epoch+1}\")\n",
    "    if experimental_setting == 8 and epoch>1: #unfreezing layer 10 for fine-tuning\n",
    "      for name, param in model.back_bone.named_parameters():\n",
    "        if name.startswith(\"encoder.layer.11\"):\n",
    "            param.requires_grad = True\n",
    "    train_loss, train_time = train_one_epoch(model, train_loader, optimizer, logging_frequency, test_loader, logger)\n",
    "    eval_acc, eval_f1, eval_loss, eval_time  = evaluate(model, test_loader)\n",
    "    logger[\"epoch_train_loss\"].append(train_loss)\n",
    "    logger[\"epoch_train_time\"].append(train_time)\n",
    "    logger[\"epoch_eval_loss\"].append(eval_loss)\n",
    "    logger[\"epoch_eval_time\"].append(eval_time)\n",
    "    logger[\"epoch_eval_acc\"].append(eval_acc)\n",
    "    logger[\"epoch_eval_f1\"].append(eval_f1)\n",
    "    print(f\"    Epoch: {epoch+1} Loss/Test: {eval_loss}, Loss/Train: {train_loss}, Acc/Test: {eval_acc}, F1/Test: {eval_f1}, Train Time: {train_time}, Eval Time: {eval_time}\")\n",
    "  \n",
    "  logger['train_time'] = logger['train_time'][1:]\n",
    "  logger['eval_time'] = logger['eval_time'][1:]\n",
    "  save_logs(logger, \"log\", str(experimental_setting))\n",
    "  save_model(model, \"models\", str(experimental_setting))\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Augment the original reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textattack.augmentation import Augmenter\n",
    "from textattack.transformations import WordSwapQWERTY\n",
    "from textattack.transformations import WordSwapExtend\n",
    "from textattack.transformations import WordSwapContract\n",
    "from textattack.transformations import WordSwapHomoglyphSwap\n",
    "from textattack.transformations import CompositeTransformation\n",
    "from textattack.transformations import WordSwapRandomCharacterDeletion\n",
    "from textattack.transformations import WordSwapNeighboringCharacterSwap\n",
    "from textattack.transformations import WordSwapRandomCharacterInsertion\n",
    "from textattack.transformations import WordSwapRandomCharacterSubstitution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word-level Augmentations\n",
    "word_swap_contract = True\n",
    "word_swap_extend = False\n",
    "word_swap_homoglyph_swap = False\n",
    "\n",
    "\n",
    "# Character-level Augmentations\n",
    "word_swap_neighboring_character_swap = True\n",
    "word_swap_qwerty = False\n",
    "word_swap_random_character_deletion = False\n",
    "word_swap_random_character_insertion = False\n",
    "word_swap_random_character_substitution = False\n",
    "\n",
    "# Check all the augmentations that you wish to apply!\n",
    "\n",
    "# NOTE: Try applying each augmentation individually, and observe the changes.\n",
    "\n",
    "# Apply augmentations\n",
    "augmentations = []\n",
    "if word_swap_contract:\n",
    "  augmentations.append(WordSwapContract())\n",
    "if word_swap_extend:\n",
    "  augmentations.append(WordSwapExtend())\n",
    "if word_swap_homoglyph_swap:\n",
    "  augmentations.append(WordSwapHomoglyphSwap())\n",
    "if word_swap_neighboring_character_swap:\n",
    "  augmentations.append(WordSwapNeighboringCharacterSwap())\n",
    "if word_swap_qwerty:\n",
    "  augmentations.append(WordSwapQWERTY())\n",
    "if word_swap_random_character_deletion:\n",
    "  augmentations.append(WordSwapRandomCharacterDeletion())\n",
    "if word_swap_random_character_insertion:\n",
    "  augmentations.append(WordSwapRandomCharacterInsertion())\n",
    "if word_swap_random_character_substitution:\n",
    "  augmentations.append(WordSwapRandomCharacterSubstitution())\n",
    "\n",
    "transformation = CompositeTransformation(augmentations)\n",
    "augmenter = Augmenter(transformation=transformation,\n",
    "                      transformations_per_example=1)\n",
    "\n",
    "\n",
    "review = \"I loved the food and the service was great!\"\n",
    "augmented_review = augmenter.augment(review)[0]\n",
    "print(\"Augmented review:\\n\")\n",
    "print(augmented_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPrediction(text):\n",
    "  \"\"\"\n",
    "  Outputs model prediction based on the input text.\n",
    "\n",
    "  Args:\n",
    "    text: String\n",
    "      Input text\n",
    "\n",
    "  Returns:\n",
    "    item of pred: Iterable\n",
    "      Prediction on the input text\n",
    "  \"\"\"\n",
    "  inputs = tokenizer(text, padding=\"max_length\", max_length=256,\n",
    "                     truncation=True, return_tensors=\"pt\")\n",
    "  for key, value in inputs.items():\n",
    "    inputs[key] = value.to(model.device)\n",
    "\n",
    "\n",
    "  outputs = model(**inputs)\n",
    "  logits = outputs.logits\n",
    "  pred = torch.argmax(logits, dim=1)\n",
    "  return pred.item()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
